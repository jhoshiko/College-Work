import numpy as np
import matplotlib.pyplot as plt

# CS 3120
# Dr. Jiang
# Joshua Hoshiko
# Created in collaboration with Gerom Pagaduan

# Generate 10 data samples
x_data = np.array([35., 38., 31., 20., 22., 25., 17., 60., 8., 60.])
y_data = 2 * x_data + 50 + 5 * np.random.random()

# print the landscape of the Loss Function (the bowl)
bias = np.arange(0, 100, 1)  # bias
weight = np.arange(-5, 5, 0.1)  # weight
Z = np.zeros((len(bias), len(weight)))

for i in range(len(bias)):
    for j in range(len(weight)):
        b = bias[i]
        w = weight[j]
        Z[j][i] = 0
        for n in range(len(x_data)):
            Z[j][i] = Z[j][i] + (w * x_data[n] + b - y_data[n]) ** 2  # this is the loss
        Z[j][i] = Z[j][i] / len(x_data)

# build linear regression model that minimizes loss using "gradient descent"
b = 0 #initial bias
w = 0 #initial weight

# test different values of the learning rate/iterations/convergence threshold values
lr = 0.00015 # learning rate
iteration = 10000 # number of iterations

# track change of the weight values from each iteration & plot it out
# Store parameters for plotting
b_history = [b]
w_history = [w]

# model generated by gradient descent
for i in range(iteration):
    b_grad = 0.0
    w_grad = 0.0
    
    for n in range(len(x_data)):
        b_grad = b_grad + (b + w * x_data[n] - y_data[n]) * 1.0 # keeps track of loss
        w_grad = w_grad + (b + w * x_data[n] - y_data[n]) * x_data[n] # keeps track of loss

    # update the bias and weight
    b = b - lr * b_grad
    w = w - lr * w_grad

    # add to history for plotting
    b_history.append(b)
    w_history.append(w)

plt.plot(b_history, w_history, 'o-', ms=3, lw=1.5,color='black')
plt.xlim(0, 100)
plt.ylim(-5, 5)
plt.contourf(bias, weight, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))
plt.plot(b, w, 'x', ms=15, c='orange')
plt.show()